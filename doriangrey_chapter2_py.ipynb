{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "doriangrey_chapter2.py",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/dorian-grey-gpt2/blob/master/doriangrey_chapter2_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "095ed744-b721-4a12-9b6a-810a9f8bfff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 25.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 29.7MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 7.8MB/s \n",
            "\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "bd81d815-59d2-4b5c-8866-f6a3115ab511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 18 02:40:09 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "5f025c56-ee40-45df-edca-dbb4535e428a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 189Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 87.6Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 283Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:23, 60.7Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 308Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 126Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 132Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "cccc9d3b-d0d1-46df-bb5f-7847a56e0c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"chapter2doriangrey.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "c46bee0b-351b-42e1-afa3-84722771affa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='chapter2-355M',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500,\n",
        "              learning_rate = 0.00001\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2449.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "dataset has 8861 tokens\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10 | 17.44] loss=2.63 avg=2.63\n",
            "[20 | 26.09] loss=2.33 avg=2.48\n",
            "[30 | 34.71] loss=2.77 avg=2.58\n",
            "[40 | 43.35] loss=2.81 avg=2.64\n",
            "[50 | 51.98] loss=2.31 avg=2.57\n",
            "[60 | 60.62] loss=2.11 avg=2.49\n",
            "[70 | 69.25] loss=1.94 avg=2.41\n",
            "[80 | 77.88] loss=1.92 avg=2.35\n",
            "[90 | 86.52] loss=1.51 avg=2.25\n",
            "[100 | 95.14] loss=1.81 avg=2.20\n",
            "[110 | 103.78] loss=1.28 avg=2.12\n",
            "[120 | 112.41] loss=1.09 avg=2.03\n",
            "[130 | 121.04] loss=1.43 avg=1.98\n",
            "[140 | 129.68] loss=0.57 avg=1.87\n",
            "[150 | 138.32] loss=0.40 avg=1.77\n",
            "[160 | 146.97] loss=0.81 avg=1.70\n",
            "[170 | 155.61] loss=0.52 avg=1.63\n",
            "[180 | 164.25] loss=0.34 avg=1.55\n",
            "[190 | 172.89] loss=0.27 avg=1.47\n",
            "[200 | 181.52] loss=0.40 avg=1.42\n",
            "======== SAMPLE 1 ========\n",
            " \n",
            "it might be time to start over.\n",
            "\n",
            "\n",
            "Harry looked at him, his eyes narrowed. \" What are you talking about ?\n",
            "\n",
            "\n",
            "\" I don't know,\" said Lord Henry. \" Perhaps we shall find some other occasion for our talking.\n",
            "\n",
            "\n",
            "\" Perhaps we shall meet again,\" said Basil. \" Perhaps we shall go on visiting one another.\"\n",
            "\n",
            "\n",
            "\" Oh Basil, I don't think so,\" said Lord Henry. \" We mustn't lose sight of what we are trying to accomplish. I I I am sure we shall always meet somewhere on the pleasant and innocent face of things. But I I I am afraid I have missed you Basil.\" He looked at Dorian Gray. \" Harry, you mustn't let your temper get the better of you,\" he said. \" It was very kind of you to call on me last Sunday, but it is always unbecoming of men to talk of things they are not going to meet. I am afraid I must leave this business of town-gardens to you. There is no business left for us.\"\n",
            "\n",
            "\n",
            "\" Harry, I am perfectly happy to stay here all season as long as you like to dine with me, and as long as the place is cool here. It is an excellent spot for a place of worship. I am sure that will please you perfectly. I only wish that you wouldn't spend your summers making up your minds in advance what you are going to say to Mr. Gray. It is rather absurd.\"\n",
            "\n",
            "\n",
            "\" It isn't absurd really Harry,\" said Lord Henry. \" It really is very absurd. But I am afraid I must go, and tomorrow I shall miss a great deal of what you have to say. I am so very sorry, Mr. Gray. I won't ever like meeting anyone like you. You are so unlike me in every way. I will always have somebody to turn to in times of need.\"\n",
            "\n",
            "\n",
            "\" Harry, I am so sorry, I really am. I really must go, and it is rather late. I really must get home. I must go to-morrow afternoon. It is almost past nine. But I I I must stay out all day. The bell rings and there comes in Mr. Gray from church after an inscrutable while a long psalm-voice demanding his attention, with its sweet notes of the church-bell, and its sweet tones of the flute, and its sweet strains of the viol and of the tambourine, and its sweet strains of the flute-cello, and its sweet notes of the viola-cello, I echo, I delirium audita, I The Bell Rises. 61 ' Toto, toto, toto, 67 The Bell Rises, The Rains,' notes of thunder-and- lightning- pour from the psalter, like the cascades of crystal-gum before a fire. And when the tempest has passed, and the pale rays of the budding sun have faded away, and the gentle sigh of spring has faded from the lips of woman like the last sweet draught from a long summer's bough, and the sweet sigh of summer's golden bough has faded into the breast of childhood like the last sweet draught from a long summer's bough, then shall us grown-ups of the tender years sound the solemn revelation, \" The Day has dawned, and the Day has ended.\" '\n",
            "\n",
            "\n",
            "Harry stepped up on the platform, and looked across at Dorian Gray, his eyes burning with the earnest earnestness of that prayer which he had just begun to make. He felt that moment as if it had been yesterday.\n",
            "\n",
            "\n",
            "\" Harry,\" cried Dorian, \" don't you come in to-morrow morning. We must go to church.\"\n",
            "\n",
            "\n",
            "\" My dear fellow, it would be unbecoming of us two really friends to meet another day. Come, let us go to church; Mr. Gray will sit with you. There is not much to do about it, Harry.\" At these words Harry turned and walked over to Basil. \" Now, Dorian,\" he said, \" let us go to church.\" \"Oh, Harry,\" said Dorian Gray, \" let us go to church.\" They walked slowly together to the church-door, and there they turned to each other. The air was solemn and solemn like that of a church. There was scarcely a breath which came from the pulpit to move the air in the church-garden. At the corner of the church-door a pretty young woman, perhaps seventeen or eighteen, was playing the lyre. Mr. Gray and Basil walked hesitatingly up and down the aisle, listening to the music. Then, suddenly, out of the corner of Mr. Gray's eye a figure appeared, followed closely by another, and another. They had all on at once become exceedingly beautiful, and there came into view before them in the open air a scene of splendor and spl\n",
            "\n",
            "[210 | 208.76] loss=0.13 avg=1.35\n",
            "[220 | 217.40] loss=0.16 avg=1.29\n",
            "[230 | 226.05] loss=0.23 avg=1.24\n",
            "[240 | 234.68] loss=0.16 avg=1.19\n",
            "[250 | 243.31] loss=0.16 avg=1.14\n",
            "[260 | 251.93] loss=0.28 avg=1.10\n",
            "[270 | 260.56] loss=0.19 avg=1.06\n",
            "[280 | 269.20] loss=0.13 avg=1.03\n",
            "[290 | 277.84] loss=0.07 avg=0.99\n",
            "[300 | 286.48] loss=0.18 avg=0.96\n",
            "[310 | 295.11] loss=0.10 avg=0.93\n",
            "[320 | 303.76] loss=0.12 avg=0.90\n",
            "[330 | 312.38] loss=0.13 avg=0.87\n",
            "[340 | 321.01] loss=0.05 avg=0.84\n",
            "[350 | 329.63] loss=0.06 avg=0.81\n",
            "[360 | 338.25] loss=0.07 avg=0.79\n",
            "[370 | 346.89] loss=0.07 avg=0.77\n",
            "[380 | 355.51] loss=0.09 avg=0.75\n",
            "[390 | 364.15] loss=0.14 avg=0.73\n",
            "[400 | 372.79] loss=0.08 avg=0.71\n",
            "======== SAMPLE 1 ========\n",
            " and it was beautiful. It was like nothing I had ever seen.\" And now a shudder ran through Allen, as he saw the smile creep across Lord Henry's face, and wave away the chill chill of the picture. \" It is dreadful stuff  \" he murmured. \" And I should like to dine with you at White Hall, Mr. Gray—at least with a cup of tea and a cigar —but my aunt won't allow it. She forbids it as often as my grandfather does.\" \" My dear fellow, I am against banishment  \" said Lord Henry. \" But I have promised to go to Paris last week, and I have promised to stay that week as well as the next. But if you can come to lunch with me at White Hall, and if you can stay as long as you like at White Hall, I shall go to Paris the next week, and stay there as long as I like the week following. Then perhaps we shall be able to come to an arrangement, which will be worth while - I assure you.\" \" I shall go to lunch with you at once. Whether I stay as long as I like or as little as I like, you can change my mind at any time. As soon as lunch-time draws nigh, I shall go out to the garden to trim some of my own adzes, and there I shall sit browsing in the garden the fruit of my own labour, and having my own idea of what a fine summer's day it is going to be what a dreadful summer's day it is going to be what a dreadful day's day will be What a wretched thing it is when passions run their course too quickly. . . . Man is astonished at the subtlety with which passions run their courses. He has never believed it possible for a thought to pass through his body sense to sense so swiftly. . . . But now that the pulse of a passion has passed its course it is possible for a thought to pass its course intuition to intuition so quickly. . . . It has never occurred to him possible that a thought should have had a course such as that. . . . The sense of colour which colours our days We are animals, Brownites, sensitive creatures. We can perceive colours. We have the faculty of colour. Let us try to conceive what it must be like to have a regular course of feeling colour. . . . What is it to have a regular course of feeling colour ? A mind at rest ? A mind conscious of its own thoughts ? A mind that never falters in its conviction that whatever comes next it must take with us, or else perish with us, or sink beneath the waves of time ? It is sheer madness for a thing to have no more time for anything but itself And to have been made for something else, the most basic of things, the very center of all reality, And to have had its beauty corrupted by senseless fancy, and thirsted after new things I Have tried to give you ideas of what it must be like to have a regular course of being filled with thoughts of your own, And to have had them run their course in your blood , And to have felt their weight on your thinking muscles To have felt their weight on your senses, and on your souls, And to have felt them grow cold on you I Have thought of your father and mother And of all your friends and loves And of the things that you have forgotten I Have thought of your brokenhearted ways and desperate longing And of the things that you have forgotten I have thought of your tears And of the things that you have forgotten You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a moment I Love you I You only remain with me A fraction of a minute I I I I Love you I You only remain with me A fraction of a minute I I I I (Whispered to him from out the brown Wreath of the carnation-trot) Speak . . . speak . . . speak . . . Tell me . . . tell me what it is like to have a regular course of feeling colour . . . To have had it purified from the impurities of fancy . . . To have had its true beauty corrupted by senseless fancy . . . To have felt its weight on your senses . . . To have felt it weight on your senses . . . To have tasted its freshness at first To have tasted its freshness at last To have tasted its freshness under another name . . . (He drew back into the little passage and stared at the brown Feathers for a long moment, then he spoke\n",
            "\n",
            "[410 | 397.74] loss=0.08 avg=0.69\n",
            "[420 | 406.37] loss=0.06 avg=0.67\n",
            "[430 | 415.00] loss=0.08 avg=0.65\n",
            "[440 | 423.62] loss=0.06 avg=0.64\n",
            "[450 | 432.25] loss=0.09 avg=0.62\n",
            "[460 | 440.87] loss=0.11 avg=0.61\n",
            "[470 | 449.51] loss=0.09 avg=0.59\n",
            "[480 | 458.12] loss=0.04 avg=0.58\n",
            "[490 | 466.76] loss=0.09 avg=0.57\n",
            "[500 | 475.38] loss=0.05 avg=0.55\n",
            "Saving checkpoint/chapter2-355M/model-500\n",
            "[510 | 491.93] loss=0.04 avg=0.54\n",
            "[520 | 500.56] loss=0.05 avg=0.53\n",
            "[530 | 509.18] loss=0.08 avg=0.52\n",
            "[540 | 517.81] loss=0.05 avg=0.51\n",
            "[550 | 526.44] loss=0.09 avg=0.50\n",
            "[560 | 535.05] loss=0.09 avg=0.49\n",
            "[570 | 543.68] loss=0.07 avg=0.48\n",
            "[580 | 552.31] loss=0.04 avg=0.47\n",
            "[590 | 560.95] loss=0.08 avg=0.46\n",
            "[600 | 569.59] loss=0.07 avg=0.45\n",
            "======== SAMPLE 1 ========\n",
            " you would have to do with him.\"\n",
            "\n",
            "\" I should have done with you if you had only lived \n",
            "that little while longer. But I didn't ; I never did \n",
            "live that little while. You have lived much longer.\" \n",
            "\n",
            "Hallward bit his lip. \" I am afraid you are right. \n",
            "You have lived too long To have been a moment's ease To \n",
            "any one. I am very glad of it.\" \n",
            "\n",
            "\" Yes, Harry, I am glad of it.\" \n",
            "\n",
            "\" Let us go and sit in the sunshine.\" \n",
            "\n",
            "They sat that minute in the cool rays, absorbed \n",
            "in the fragrance of the roses, the sound of the \n",
            "birds, the murmur of the beasts around them. \n",
            "Suddenly one of them said, \" I wonder \n",
            "if your father and I are really quite as silly as \n",
            "we seem to be. We sit like that all the time, and \n",
            "don't wake up. Are we really as silly as we \n",
            "look? Have we really never thought Of anything but \n",
            "to sit like that And think Of nothing But sitting And \n",
            "thinking Our whole lives We would It' \" \n",
            "\n",
            "Hallward's eyes fixed fixedly on the window- \n",
            "piece, and he seemed to be thinking something \n",
            "aside. When he had spoken, they went \n",
            "and opened the case of the camera and set it \n",
            "down on the easel. There was a tinge of perspiration \n",
            "to the picture-glass, but it made the scene \n",
            "very singular. There was not a leaf, stone, or insect \n",
            "that moved. The only sound was the soft, melodious \n",
            "loudness of the shutter. The painting seemed to be \n",
            "dragging on the page, and, at every moment, to be \n",
            "taking a step out of view. But then the \n",
            "painting seemed to be taking a step \n",
            "for the foreground, and the foreground \n",
            "step was the picture. It was the picture of \n",
            "John Maynard Keynes As seen by Erich Fromme In A \n",
            "real life-painting The picture-image In The Picture- \n",
            "gallery As painted It really Is perfectly clear To Rembrandt \n",
            "Sketch Who has really finished It And What it will Show \n",
            "He likes It very much, but he doesn't like To Hadit He \n",
            "Don't like It at all, and He'll tell Nobody What He \n",
            "Has DONE With It<|endoftext|>This article originally appeared on VICE Germany.\n",
            "\n",
            "\n",
            "German news media are often quick at reporting revelations about the European Union from the inside out. It's one of the reasons that the first thing you learn about the UK when you're a 12-year-old is that it can't have its cake and eat it, too. Every government mouthpiece in the country has been quoting from reports, leaked from spy agencies or leaked from the intelligence services themselves. But what they always forget to tell you is that all this reporting is actually just about to get a lot worse.\n",
            "\n",
            "It's been a busy summer for Edward Snowden. The Russian computer whiz has been living in Hawaii, flying home to the USA for a week and leaving a trove of sensitive documents behind. He is believed to have taken three laptops – one of them his – with him: the first model, sent him by a courier in Hong Kong, containing a report on the US National Security Agency's global surveillance programme; the second, stored in a rented flat in the capital, was stolen inside the night of 18–19 May. The third, which he left in Russia, had been delivered to his hotel in Hawaii two days earlier by a member of the Russian police, in response to a Freedom of Information Act (FOIA) request.\n",
            "\n",
            "This week the Guardian reported that, while holed up in Hawaii, Snowden had sent a series of encrypted e-mails to Laura Poitras, the acclaimed filmmaker, and to Glenn Greenwald, a partner at the newspaper. In one, he had promised to provide them with information about a British police operation called Tempora, in which hundreds of thousands of telephone records and other data had been collected and analysed. Poitras and Greenwald have been working on this series of stories for months, together in the shadow of Hong Kong.\n",
            "\n",
            "Both men are passionate journalists, but the stakes are particularly high in the age of the internet. The release of these e-mails and of the Tempora files, and the exposure as well of the vast scale of the US mass-surveillance programme, are intended to bring this country to its knees.\n",
            "\n",
            "In the past three years the security services have revealed a tremendous amount about people's private lives. They have also revealed very little about what we actually want from them.\n",
            "\n",
            "It is a classic balance: it is understandable why terrorists want to be known only as lone wolves. But\n",
            "\n",
            "[610 | 594.87] loss=0.04 avg=0.44\n",
            "[620 | 603.50] loss=0.05 avg=0.43\n",
            "[630 | 612.16] loss=0.04 avg=0.43\n",
            "[640 | 620.79] loss=0.05 avg=0.42\n",
            "[650 | 629.41] loss=0.05 avg=0.41\n",
            "[660 | 638.06] loss=0.06 avg=0.40\n",
            "[670 | 646.71] loss=0.06 avg=0.40\n",
            "[680 | 655.36] loss=0.05 avg=0.39\n",
            "[690 | 663.99] loss=0.04 avg=0.38\n",
            "[700 | 672.63] loss=0.05 avg=0.38\n",
            "[710 | 681.27] loss=0.05 avg=0.37\n",
            "[720 | 689.91] loss=0.04 avg=0.36\n",
            "[730 | 698.54] loss=0.07 avg=0.36\n",
            "[740 | 707.16] loss=0.06 avg=0.35\n",
            "[750 | 715.80] loss=0.08 avg=0.35\n",
            "[760 | 724.43] loss=0.04 avg=0.34\n",
            "[770 | 733.08] loss=0.05 avg=0.33\n",
            "[780 | 741.73] loss=0.03 avg=0.33\n",
            "[790 | 750.36] loss=0.05 avg=0.32\n",
            "[800 | 758.98] loss=0.03 avg=0.32\n",
            "======== SAMPLE 1 ========\n",
            " I remember a time when we were all so young The stars had never been born, the moon had never been wax'd, The winds had not stirred the treetops, The birds had not crowed, and the beasts Had not been creatures at all! \n",
            "\n",
            "CHAPTER XXI\n",
            "\n",
            "SIR, YOUR WONDER YET IS COMPELLING YOU. YOU HAVE SEEN IT BEFORE. YOU HAVE TOLD ME. AND YET YOU FAIL. YOU FAIL, BECAUSE YOU HAVE NEVER HAD A REAL PHANTOM. YOU HAVE A REAL PHANTOM, A REAL THIEF, AN ABSURD FAULTY, WHO IS FRUSTRATED, AN ABSTAINENT, AN AUDITION-ROCKETED VOICE, WHO ISN'THING AT ALL. \n",
            "\n",
            "THE MORBID SILENCE was broken by the CHORUS OF RUTH. \n",
            "\n",
            "\" Yes,\" she murmured, \" I am quite finished with you, Basil. And I shall wager a good deal of money that the Jell- O-Matic doesn't make your job any worse.\" \n",
            "\n",
            "\" That will do nicely,\" said Lord Henry. \" I'll go in to look after Mr. Gray, and when I get home to you \n",
            "I shall send for you via post a copy of Mr. Gray's \"That is, a current of violets- \n",
            "and \n",
            "sixth-of-octoe- \n",
            "\n",
            "night's steaming-hot- \n",
            "\n",
            "news.\" \n",
            "\n",
            "\" That will do quite nicely, Harry,\" said \n",
            "Hallward. \" And call on me when you are fairly through with \n",
            "it, dar- mar. That is, when you are quite through with noth- \n",
            "ing at all.\" \n",
            "\n",
            "\" Oh, I will be straight with you, Harry,\" said \n",
            "Lord Henry. \" That is what I always like to see incom- \n",
            "prehensible wisdoms come to man. The artist is able \n",
            "to give a beautiful work the hideous name of brutality, \n",
            "but the public are afraid of brutality, and cannot stand it. \n",
            "They want true art, something more than blood and \n",
            "poison. I am perfectly satisfied with my standard by \n",
            "which I measure our present temperance. It gives me \n",
            "some satisfaction to know that as you were rising in the \n",
            "light I could say to you: \" Don't you appreciate that \n",
            "beauty when in its true sense it gives you terror ? \" Or : \" Don't you \n",
            "see that by your following the path I have marked you will return \n",
            "to your own barbarism ? \" Or: \" Mr. Gray, don't you \n",
            "always run your own course, and follow only my example ? \" \n",
            "\n",
            "\" Mr. Gray, you will always follow mine,\" said \n",
            "Lord Henry. \" I command you. I act upon the \n",
            "principle that if you do not comply with my will, \n",
            "then to punish you is to punish me ? \" \n",
            "\n",
            "\" I should have his head,\" cried Mr. Gray, uproar- \n",
            " \n",
            "\n",
            "ing. \" If it is legal for me to punish a mentally-ill person- \n",
            "ing, then , Lord Henry, to me it is just as \n",
            "legal a matter as it is to run a marathon and return empty- \n",
            "handed. It is your will, Mr. Gray, and I have the \n",
            "right to refuse it. If it is against my will, then so am I \n",
            "and so can you. Keep to the will, and you shall get \n",
            "it. If it seems to you that you have got it, keep to the \n",
            "will, and it shall seem to your loved one that you have a \n",
            "little something more. Or, if you should venture to depart \n",
            "from the will, you shall meet with hostile people for- \n",
            " yning to speak against it. They will question your honour, \n",
            "and your family. You will be hated by them for speaking \n",
            "against their leader. You will be misunderstood. People \n",
            "will think you had it better not to say anything atall, \n",
            "or they would throw you out of the race. Or, if you \n",
            "do not wish to be misunderstood, you can always go ahead and \n",
            "speak your mind. That is what Governments are \n",
            "designed to do. They make us slaves to our opinions. \n",
            "\n",
            "Mr. Gray, I am going to go in to look after you. Will \n",
            "you send word back to me when you are going in ? I \n",
            "want you to be up very early to-morrow morning to- \n",
            "morrow morning for that purpose. I want you to make yourself \n",
            "at home at Whitefriars. The neat little neighborhood in which \n",
            "\n",
            "[810 | 784.12] loss=0.03 avg=0.31\n",
            "[820 | 792.75] loss=0.03 avg=0.31\n",
            "[830 | 801.37] loss=0.04 avg=0.30\n",
            "[840 | 810.01] loss=0.03 avg=0.30\n",
            "[850 | 818.65] loss=0.05 avg=0.29\n",
            "[860 | 827.30] loss=0.04 avg=0.29\n",
            "[870 | 835.94] loss=0.03 avg=0.29\n",
            "[880 | 844.56] loss=0.03 avg=0.28\n",
            "[890 | 853.20] loss=0.03 avg=0.28\n",
            "[900 | 861.84] loss=0.04 avg=0.27\n",
            "[910 | 870.47] loss=0.08 avg=0.27\n",
            "[920 | 879.11] loss=0.02 avg=0.27\n",
            "[930 | 887.73] loss=0.05 avg=0.26\n",
            "[940 | 896.36] loss=0.04 avg=0.26\n",
            "[950 | 904.97] loss=0.04 avg=0.26\n",
            "[960 | 913.62] loss=0.05 avg=0.25\n",
            "[970 | 922.26] loss=0.03 avg=0.25\n",
            "[980 | 930.90] loss=0.04 avg=0.24\n",
            "[990 | 939.54] loss=0.03 avg=0.24\n",
            "[1000 | 948.17] loss=0.03 avg=0.24\n",
            "Saving checkpoint/chapter2-355M/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "outputId": "01d3c4a5-8c88-46cb-a661-11b076b4f47d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='chapter2-355M')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b678aeb0b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_checkpoint_to_gdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'chapter2-355M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gpt2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='chapter2-355M')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "2c7d200a-cf1d-4c9d-8508-0682e5149f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='chapter2-355M')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/chapter2-355M/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/chapter2-355M/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "585a3b75-537e-49f1-90af-f5cca4c730ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='chapter2-355M')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I was reading an article about the psychology of diet when I suddenly remembered a letter I had received from Dorian Gray. It had been written some months since to a girl who was engaged to be married. It was an affectionate letter, in which the author hoped that the feelings it aroused would be of some use to his intended recipient. He had written back almost immediately to say that he had not been able to receive it. He had seemed so perfectly happy when he had written it. I wonder if his feelings on the subject of youth have ever really mellowed \n",
            "him out ? \"\n",
            "\n",
            "\" I certainly hope so.\" \n",
            "\n",
            "\" Ah, well, I don't think they have.\" \n",
            "\n",
            "There was a long silence. \n",
            "\n",
            "\" If they have,\" said Lord Henry, \" I shall never marry.\" \n",
            "\n",
            "\" That is entirely out of order, Basil.\" \n",
            "\n",
            "Hallward turned to him. \" You are terribly dull-witted, Mr. Gray. You bade me sit down. I didn't ask you to come. But I welcome your approval. It is certainly better than being disliked. Mr. Gray, I really must go.\" \n",
            "\n",
            "\" Basil, I am sick of standing around like that. Let me out \n",
            "and tell my story.\" \n",
            "\n",
            "\" You know you mustn't, Harry. If you go in person, he will think \n",
            "of laughing at you, and you will lose your temper. And Basil, I don't \n",
            "want to be misunderstood. I know exactly what you are about.\" \n",
            "\n",
            "\" You are not,\" returned the painter, coldly, when he had \n",
            "left. \n",
            "\n",
            "Lord Henry went out to the garden, and found himself face to \n",
            "face with his portrait. The boyish face and the earnest gaze \n",
            "that had been growing on him, had made up one of the most \n",
            "possessible features of his century. He was certainly no \n",
            "wonder he should have found his portrait so suitable. \n",
            "\n",
            "\" I am leaving thither to-morrow, Lord Henry. I am awfully \n",
            "busy these days, trying to look recent. I must \n",
            "be careful not to forget anything. I have just come \n",
            "from seeing the new exhibition at Whitefriars. I wanted \n",
            "to stay longer at the Court of Vase, but the painter has \n",
            "made me stay too long. I don't know what to do. I want \n",
            "to be alone. Is it because I am late? or because you \n",
            "haven't come \n",
            "\n",
            "anymore ?\" \n",
            "\n",
            "\" Neither of those things makes any difference.\" \n",
            "\n",
            "\" Ah I \n",
            "\n",
            "\" I am so dull-witted. I don't know what to \n",
            "think anymore. I feel as if I had never been born. \n",
            "Pain! that is what stays. It gnaws at my soul \n",
            "every moment that goes by. I know I must get up \n",
            "and go out to the garden, but I don't know what \n",
            "to do. I feel as if I had never lived. I feel \n",
            "as if I had lived forever. . . . Yes, Harry, that \n",
            "is what it must do. . . . You will always \n",
            "have Harry, Basil. You have always had Harry. \n",
            "Laugh at your own silly humor, and it will be true \n",
            "to you. Watch over your father, and he will protect \n",
            "you. Remember what you have said, and you will never \n",
            "be sorry for a moment. . . . Yes, Dorian, that is \n",
            "what marriages are for. Every marriage is a struggle \n",
            "for the best influence possible. Every boy who grows up \n",
            "in this country is led to it by the favorite game of \n",
            "his childhood: to be the darling of the neighborhood, \n",
            "to be taken under the wing of a favorite toy, and, if possible, \n",
            "to learn to love it. It Is a terrible work, that art. \n",
            "It Is the vulgarification of life. Let us give it up. \n",
            "Let us end it. Of course, once it has been officially \n",
            "approved, it cannot be changed. But what harm \n",
            "can it do, that people should change their opinions ? \n",
            "\n",
            "That is what I am going to do. I am going to change \n",
            "my mind. When I am a grown man, when I am a \n",
            "famous author, when I am a citizen of a world- \n",
            "wide city, when the gods have forgotten me, and I am \n",
            "just another strumpet on a pyre of a dying faith, then I \n",
            "will either be true to my feelings, or false to them. \n",
            "Or, the gods will adjust the terms of my death, \n",
            "and make me choose the one miserable part\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "1f34f84e-32e0-4a02-b8c4-d7aae7ce984c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              run_name = 'chapter2-355M',\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"Nothing can cure the soul but the senses\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nothing can cure the soul but the senses.\" \n",
            "\n",
            "Philosopher \n",
            "\n",
            "VI. \n",
            "\n",
            "\" You ask me,\" said Lord Henry, \n",
            "looking at him with his dreamy, languorous \n",
            "eyes, \" have you not ? \" \n",
            "\n",
            "Dorian Gray frowned, and turned and walked away. \n",
            "After a few steps, Lord Henry called out to him. \n",
            "\" Hey, Dorian, what are you doing go and tell Fawl \n",
            "Artwork to come home to you ? \" \n",
            "\n",
            "Dorian Gray turned and saw that his usual stooping \n",
            "gait was broken. He turned again and saw that the \n",
            "poison had made its appearance. A trail of thick \n",
            "black mist had it to lead. \n",
            "\n",
            "\" You are stinking funny, Basil,\" said the painter, \n",
            "bitterly. \n",
            "\n",
            "The lad started and looked at his feet. \" I \n",
            "don't want to be stinky,\" he murmured. \n",
            "\" You are immensely foul smelling,\" cried the \n",
            "painter, in horror. \n",
            "\n",
            "\" Amaze- \n",
            "\n",
            ",\" answered Lord Henry. \" And I don't \n",
            "wish that I could, Dame\n",
            "====================\n",
            "Nothing can cure the soul but the senses ; \" \n",
            "\n",
            "\" Or, again, may I have the honour of judging ? \" \n",
            "\n",
            "4 The old gentleman looked at him. Yes, that was all that mattered ; he was \n",
            "there by his creator's licence. A dirty old courtier had \n",
            "slipped a ducat on him, and it had been meaningless to protest ; \n",
            "now that title was ali that mattered. The painter had \n",
            "taken a dirty fancy to the lad, and had broken off \n",
            "their acquaintance by joking. How ridiculous that ridiculous thing \n",
            "had been ! He had merely irritated him. Why should he \n",
            "have liked it ? The picture had been stolen from him. It \n",
            "was worthless to have a picture of himself. Nothing could be \n",
            "so simple as that. \n",
            "\n",
            "\" Mr. Gray, I fancy you would like The God of Small \n",
            "Diminishes * * * so much, if it were only there was some way \n",
            "of making it stay ?\" \n",
            "\n",
            "\" There is,\" said Harry, staring \n",
            "at the picture. \" But, Lord Henry, if The \n",
            "Gods have wills, what great powers have they ! \n",
            "\n",
            "Let\n",
            "====================\n",
            "Nothing can cure the soul but the senses. . . . You speak too softly of \n",
            "the \n",
            "\n",
            "soul.\" \n",
            "\n",
            "\" I am afraid you are wrong,\" \n",
            "Dorian Gray said. \n",
            "\n",
            "Lord Henry glanced at him. Yes, there was something in his brown eyes that \n",
            "seemed to say to him, Mr. Gray, if only you would stop talking you\n",
            "would have some pleasure. \" I will try to stop talking at least,\" \n",
            "he said. \" If it is all right, I willv Talk to-morrow morning.\" \n",
            "\n",
            "Dorian Gray nodded, and walked over to the deal painting-table \n",
            "that was set beneath the high curtained window. He \n",
            "kept his eyes shut, and when he opened them a \n",
            "stream of tears came to his eyes. He was \n",
            "glad he had not started at once, and wished that he had \n",
            "told him what to do. But it was not necessary. \n",
            "As he sat there looking at him, some dreadful scene \n",
            "was playing out in the studio-chamber below. A \n",
            "Dark Prince was dancing in the air, and music \n",
            "was playing \n",
            "among the squaws and the vix\n",
            "====================\n",
            "Nothing can cure the soul but the senses.\" \n",
            "\n",
            "\" But the soul is blind 1 \" \n",
            "\n",
            "\" It is so right you should be \n",
            "blind.\" \n",
            "\n",
            "There came a knock at the door, and the butler \n",
            "entered with a laden tea-tray and set it down upon \n",
            "a small Japanese table. There was a rattle of cups \n",
            "and saucers and the hissing of a fluted Georgian urn. \n",
            "Two globe-shaped china dishes were brought in by \n",
            "a page. Dorian Gray went over and poured out the \n",
            "tea. The two men sauntered languidly to the table, \n",
            "and examined what was under the covers. \n",
            "\n",
            "\" Let us go to the theatre to-night,\" said Lord \n",
            "Henry. \" There is sure to be something on, some- \n",
            "where. I have promised to dine at White's, but it \n",
            "is only with an old friend, so I can send him a wire \n",
            "to say that I am ill, or that I am prevented from \n",
            "coming in consequence of a subsequent engagement. \n",
            "I think that would be a rather nice excuse : it would \n",
            "\n",
            "====================\n",
            "Nothing can cure the soul but the senses.\"\n",
            "\n",
            "\" I am in love with you both \" cried Dorian Gray. \" I am in love with life \" and with youth \" \n",
            "\n",
            "the first one since he was born \" \n",
            "\n",
            "\" My dear fellow, you are too charming for Paradise. You would ruin it instantly. You would destroy both. But let us not let anyone tell us that one is better than the other. \n",
            "\n",
            "Live! Live happily, as far as the trees can tell you to live. Let the breeze of youth \n",
            "blow over your tomb-stones and perfume your park-leaves. Be born again \n",
            "or you will suffer horribly in the dual hell of sensuality and \n",
            "materialism. Each features merely as an echo in the other's ear. \n",
            "One dies of hunger, the other of want. The world loses one \n",
            "for violence, and gives another to ignorance. Each delights in the \n",
            "surrender of the other. Each ends by sacrificing his soul to the \n",
            "technique of love. You will suffer for only a moment what \n",
            "it will take for centuries to grow weary of your lovely light. \n",
            "One will laugh at you for loving not one, but two,\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}