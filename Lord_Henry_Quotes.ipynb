{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lord Henry Quotes",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seyonechithrananda/dorian-grey-gpt2/blob/master/Lord_Henry_Quotes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "3aa29cef-fd53-4797-e465-8d4b18a5e892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "231cdfd6-20af-4646-e155-3065cc9d09d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 20 00:27:22 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "19338c6b-b73a-4b8f-f9e7-e5bd14f3fbee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 260Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 120Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 449Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:10, 133Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 344Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 101Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 138Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "447f9cbc-454e-4f5d-e58c-7c584f1d0362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"lordhenryrants.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "a2aa9812-0934-422b-c9c6-ba18f051dedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='lordhenry-355M',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500,\n",
        "              learning_rate = 0.00001\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 234.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "dataset has 3778 tokens\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10 | 19.25] loss=2.74 avg=2.74\n",
            "[20 | 28.07] loss=2.42 avg=2.58\n",
            "[30 | 36.85] loss=2.88 avg=2.68\n",
            "[40 | 45.64] loss=2.11 avg=2.54\n",
            "[50 | 54.45] loss=1.69 avg=2.36\n",
            "[60 | 63.24] loss=1.50 avg=2.22\n",
            "[70 | 72.02] loss=0.68 avg=1.99\n",
            "[80 | 80.83] loss=0.86 avg=1.84\n",
            "[90 | 89.63] loss=0.41 avg=1.68\n",
            "[100 | 98.44] loss=0.27 avg=1.53\n",
            "[110 | 107.23] loss=0.17 avg=1.40\n",
            "[120 | 116.04] loss=0.09 avg=1.29\n",
            "[130 | 124.84] loss=0.09 avg=1.19\n",
            "[140 | 133.64] loss=0.14 avg=1.11\n",
            "[150 | 142.46] loss=0.08 avg=1.04\n",
            "[160 | 151.28] loss=0.10 avg=0.97\n",
            "[170 | 160.08] loss=0.57 avg=0.95\n",
            "[180 | 168.89] loss=0.07 avg=0.89\n",
            "[190 | 177.68] loss=0.08 avg=0.85\n",
            "[200 | 186.48] loss=0.07 avg=0.80\n",
            "======== SAMPLE 1 ========\n",
            " is a thing to be desired for nothing,\" says Pynchon, whose latest book, \"The Old Man and the Sea,\" has just been published. I want you to think of some other man. Of course you would like him more. You would adore him. He is the true perfection of man. He is your ideal. He should be your idol.\n",
            "\n",
            "Yes. That would be splendid. I should be obliged, of course, to comply with every impulse which rose in me to indulge in such indulgences as these. But there is something in them which amaze me, and which, if I may so speak, keep me from living up to my potentialities. Sometimes I wonder if the gods really play dice with the people of this land. Perhaps they don't.\n",
            "\n",
            "Yes. I think that is the whole joke of him. There are only two types of men. There are those who can be got by playing dice, and there are those who can't be got by playing dice. No matter what the dice score, when they are done they make a new man. All the rest, what is it but a Parma birth? What makes you think of that town, Rome? Of course you won't go there. That would be blasphemy. But why bother? I'll prove it to you. When you are old and grey and ugly and miserable you'll replace them with somebody new whom you can easily be pleased with. A hundred per cent success! That's what they want. You should be very happy with it.\n",
            "\n",
            "You mustn't say the things you say to him. Sometimes, of course, they are not true. I don't mean to say that. There are things I don't know, or am unwilling to know, or am afraid to say, or do. But there are also things I am quite certain I am not going to be asked about in twenty years' time. Any man who tries to be interesting in his forte without being absolutely sure he doesn'tWant to lose, Mr. Gray, is trading on passion for objectivity. The only way to get to the truth is to be brutally honest with oneself. Play to your true feelings. Live your passions to the full. Heaven help you who do not realize the spiritual in passion. The better to serve your passions, your real loves, are to be avoided. Perfection is without a soul. It is the inevitable product of the fiery intellectual passions which go with them.\n",
            "\n",
            "You are a marvelous picturewriter. I take pleasure in destroying records. I don't like new ones. I have destroyed thousands of them. You, Mr. Gray, who is hardly younger than I am, can probably write a novel which will make the greatest living poet of all time laugh his grave, miserable head off in his sleep, and to this day the greatest living poet of all time will be a richer man for having read The Lord of the Rings than he will be for having read the Bible. The only way to get rid of a poet is to give him the name of the Empress Caligula. There is nothing like her in the world. Try your luck. I shall love you always, and will love you always, because there is no other way.\n",
            "\n",
            "Yes. There was. That was the second vision. I remember thinking—I think it was the eighth vision—and thinking like a drunken mountain, as I did, when I attained to them, that there must have been lights under the pavement, and that was the best explanation of everything that had happened. I really can't recollect what the twelfth vision was but a vague idea. It must have been some wonderful dream, like the one Mrs. Grey had had. I don't know what it was, but it seemed to me there had been something monstrous or marvellous or wonderful going on under the pavement some day, and there had not been, and there always will be. I always fancy that whenever I am in the Ministry I have glimpsed that terrible moment when God makes his appearance to ask our pardon for our sins, but says nothing of the terrors of which we are but the victims. When I reach the age of majority I can get rid of it simply by turning my thoughts from sin to sin, and reminding God of the ones that were past and the neverwasters of the future. That is what I have been doing. That was the second vision. I don't mean to say that God has been angry with me, or that he has been mean to me, or that I have been ungrateful to him. On the contrary, He has been kind to me. He has always been kind to me. I don't think he knows that I am Gay. He knows that I am Haig. We are both Haigers. Sometimes He laughs, and He is quite charming, and sometimes He makes us sad, and He is quite horrid, and sometimes He makes us laugh, and we always laugh, and that\n",
            "\n",
            "[210 | 215.71] loss=0.05 avg=0.76\n",
            "[220 | 224.50] loss=0.06 avg=0.73\n",
            "[230 | 233.31] loss=0.09 avg=0.70\n",
            "[240 | 242.10] loss=0.03 avg=0.67\n",
            "[250 | 250.90] loss=0.04 avg=0.64\n",
            "[260 | 259.71] loss=0.09 avg=0.61\n",
            "[270 | 268.48] loss=0.07 avg=0.59\n",
            "[280 | 277.26] loss=0.04 avg=0.57\n",
            "[290 | 286.07] loss=0.07 avg=0.55\n",
            "[300 | 294.86] loss=0.06 avg=0.53\n",
            "[310 | 303.66] loss=0.02 avg=0.51\n",
            "[320 | 312.45] loss=0.03 avg=0.49\n",
            "[330 | 321.23] loss=0.02 avg=0.48\n",
            "[340 | 330.02] loss=0.05 avg=0.46\n",
            "[350 | 338.81] loss=0.05 avg=0.45\n",
            "[360 | 347.62] loss=0.05 avg=0.44\n",
            "[370 | 356.42] loss=0.04 avg=0.42\n",
            "[380 | 365.22] loss=0.06 avg=0.41\n",
            "[390 | 374.03] loss=0.04 avg=0.40\n",
            "[400 | 382.81] loss=0.04 avg=0.39\n",
            "======== SAMPLE 1 ========\n",
            " when a baby is born, he never feels anything. He just sits there feeling quite contented. When a man is old and dull, he always looks like an idiot. That is the mark of a stupid man.\n",
            "\n",
            "Adultery is the great sin. But for that you cannot understand what is going on in there. When my wife and I were married, we didn't have an internet. We used to drop by the library to gaze at a picture, or listen to some book, or do something that would make us happy. It was as if we had forgotten that we were human. That is what sin is. it is that horrible word that makes us feel terrible\n",
            "\n",
            "Well played to the bitter, that was what he called himself after. Vain! That was what he had been all his life. Vain was what he had WANTED. Vain was everything that had been wrong with him. For some reason, he'd always insisted that his sins were sins of taste. That was the one thing he'd NEVER been. He'd always been a victim.\n",
            "\n",
            "He was being hilariously foolish by admitting that he has it in for her. Of course he does. He knows her. They know each other. They're married. She is having an enjoyable day. He is having an unpleasant one. They are out having an unpleasant time. It's a mutual sensation. That's the sort of girl he is. That's the sort of guy he is. They are, really, the most inimitable forms of life.\n",
            "\n",
            "Well played to the bitter, that was what he was called after. Well played he was. Well played he has been to the past. Well played he has never been to the present.\n",
            "\n",
            "\n",
            "Bill Hicks lived in the year 2029.\n",
            "\n",
            "The present Miss Hicks lives in the year 2154.\n",
            "\n",
            "\n",
            "Adultery is the great sin. But for that you cannot understand what is going on in there. When my wife and I were married, we didn't have an internet. We used to drop by the library to stare at a picture, or listen to some book, or do something that would make us happy. It was as if we had forgotten that we were human. That is the mark of a stupid man. They are the saints. They are the divine. They have nothing to do. That is the penalty for trifling sins. That's the whole point of the world. There is no business left to be done. Life is a series of minor sins. As long as man goes about doing nothing but sighing and watching the sunset, he is in league with nature. She wins, and they score.\n",
            "\n",
            "\n",
            "Well played to the bitter, that was what he was called after. Vain! That was what he had been all his life. Vain was what he had WANTED. Vain was everything that was wrong with him. For some reason, he'd always insisted that his sins were sins of taste. That was the one thing he'd NEVER been. He'd always been a victim. He had no future. He was going to die. He was going to sit there in there, feeling utterly useless, watching the sun go down the west coast of Africa and thinking to himself: 'Now that I've got her, she loves him! He'll be just like him! Oh, shit! He's got her!'\n",
            "\n",
            "\n",
            "Well played to the bitter, that was what he was called after. Vain! That was what he had been all his life. Vain was what he had WANTED. Vain was everything that was wrong with him. For some reason, he'd always insisted that his sins were sins of taste. That was the one thing he'd NEVER been. He'd always been a victim. He had no future. He was going to die. He was going to sit there in there, feeling utterly useless, watching the sun go down the west coast of Africa and thinking to himself: 'Now that I've got her, she loves him! He'll be just like him! Oh, shit! He's got her!'\n",
            "\n",
            "\n",
            "Well hit me right between the eyes...\n",
            "\n",
            "\n",
            "Adultery is the great sin. But for that you cannot understand what is going on in there. When my wife and I were married, we didn't have an internet. We used to drop by the library to stare at a picture, or listen to some book, or do something that would make us happy. It was as if we had forgotten that we were human. That is the mark of a stupid man. They are the saints. They are the divine. They have nothing to do. That is the salutary effect of having fun!\n",
            "\n",
            "\n",
            "Well played to the bitter, that was what he was called after. Vain! That was what he had been all his life. Well played he was. Well played he had been to the past. That is the mark of a stupid man. They are the saints\n",
            "\n",
            "[410 | 409.10] loss=0.03 avg=0.38\n",
            "[420 | 417.89] loss=0.02 avg=0.37\n",
            "[430 | 426.68] loss=0.05 avg=0.36\n",
            "[440 | 435.47] loss=0.02 avg=0.35\n",
            "[450 | 444.28] loss=0.04 avg=0.34\n",
            "[460 | 453.07] loss=0.03 avg=0.33\n",
            "[470 | 461.89] loss=0.06 avg=0.33\n",
            "[480 | 470.69] loss=0.06 avg=0.32\n",
            "[490 | 479.48] loss=0.05 avg=0.31\n",
            "[500 | 488.29] loss=0.05 avg=0.30\n",
            "Saving checkpoint/lordhenry-355M/model-500\n",
            "[510 | 505.33] loss=0.04 avg=0.30\n",
            "[520 | 514.12] loss=0.05 avg=0.29\n",
            "[530 | 522.91] loss=0.02 avg=0.29\n",
            "[540 | 531.72] loss=0.02 avg=0.28\n",
            "[550 | 540.51] loss=0.04 avg=0.27\n",
            "[560 | 549.31] loss=0.03 avg=0.27\n",
            "[570 | 558.11] loss=0.02 avg=0.26\n",
            "[580 | 566.90] loss=0.02 avg=0.26\n",
            "[590 | 575.70] loss=0.02 avg=0.25\n",
            "[600 | 584.51] loss=0.03 avg=0.25\n",
            "======== SAMPLE 1 ========\n",
            "iving people of any age, of any sex, are naturally attracted to one another. The only difference between us is that we are made of different metals.\n",
            "\n",
            "I really could not fail to make you my bride, should I be permitted to do so. I have not the least conception what the laws of romance are, or what the true motives of youth are. But, whatever the secrets of the human heart, they lie veiled in plain language and solemn solemn ritual. As for the beautiful, the true creatures of passion, they are not worth a sultry evening with their adoring families. They only stir the wild passions of the soul.\n",
            "\n",
            "I never really loved anyone. I merely passionately desired to be loved. I have nothing to regret. I have everything to smile on. Live! Live the life that you know not what happiness is supposed to be. That is the only real happiness.\n",
            "\n",
            "E*UGH* I am afraid that you must confess, Mr. Gray, that your early years were an unhappy one. You grew quite fat and very foolish, and all the while that dreadful old house in the Park, where the poor creatures who live there are always crowded together in huge, dismal little cottages, you felt as if you had come from outside the pale of the world and were walking in its filth and squalor. Great weight had indeed fallen upon your frame,--your soul was weighed down with its own! But that is nothing compared with the grief and the shame that you must have must have felt! For the space of some years, until you were quite of age, you must have have have hashed your pearls into little bundles, and pared your pearls with your axe. When you grew up you will have thought the sight of a pretty young woman hideous, and will have felt the touch of a mighty mouse-crow. There is nothing you can do but be wretched all your life. At least, that is what the rich people are supposed to be doing. The poor people are simply wasting away in abject poverty. They are dying out, and it is you, Mr. Gray, who are the only people left who will ever remain undisturbed. You will be loved and you will be had. The moment you step forth from the shadows, you shall have them. You will be the things that men are not, and things that they are not. You will be, when they are with you, they are with you, and when they are with you they are with you, too.2\n",
            "\n",
            "There was something terribly queer and wonderful in this picture. As soon as you had put it down, you pressed it forward again, and again, and again, and instantly you had blighted the picture with a hideous, ghastly, overgrown colour. You had plastered it with blushes and patches and hues which the fire had deified and merited butign from your pictures. You had lavished upon it a subtle, a sweet, and a arosy radiance which the fire had denied to divine and pure and sincere. The moment you had looked at it, your soul had burst into flame with divine flame, and your eyes with sweet, sweet, unvarying fire. You have never been able to resist looking at things divine and good, you wonderful creature! You havelicced them with your own sweet, soft, unpretending, innocent, divine radiance, you have made their faults your own, your virtues your own, your successes your own! And when you have destroyed their souls, when you have bruised their bones, when you have destroyed their imaginations, when you have made their days vain and their nights wretched, you will always be loved and will always be had. All your life you have had, you will have had, and with each day that passes by you grow weaker and weaker. At last you will be as one of those poor victims, put out of their picture by the fire, and left to rot in its obscurity. Will you feel any pain? Will you feel anything but love for the beautiful things in the picture? Then you are truly the things that men are not, and things that men are not are dreadful places to be in. Dry, damp, and lonely, they are the best types that Nature has for mice. Metamorphose them into something beautiful, and they immediately lose their souls. Coloreds are in the end mere numbers, and are therefore purely aesthetic. Flowers are mere colors, which makes them all the more essential. But words are too precious an asset for beauty to beought on earth, and are therefore purely functional. So, rather than give up the struggle, I have been spending the better part of the past several months working out a system of color-mythology. It has been a pretty success, and I am immensely touched.\n",
            "\n",
            "The lady in the fairy-carpet was so pretty that I fancied I had struck gold. But why should I write,\n",
            "\n",
            "[610 | 610.80] loss=0.03 avg=0.24\n",
            "[620 | 619.60] loss=0.04 avg=0.24\n",
            "[630 | 628.40] loss=0.03 avg=0.23\n",
            "[640 | 637.19] loss=0.04 avg=0.23\n",
            "[650 | 645.98] loss=0.02 avg=0.23\n",
            "[660 | 654.79] loss=0.02 avg=0.22\n",
            "[670 | 663.57] loss=0.04 avg=0.22\n",
            "[680 | 672.38] loss=0.03 avg=0.21\n",
            "[690 | 681.16] loss=0.02 avg=0.21\n",
            "[700 | 689.97] loss=0.04 avg=0.21\n",
            "[710 | 698.75] loss=0.04 avg=0.20\n",
            "[720 | 707.55] loss=0.03 avg=0.20\n",
            "[730 | 716.32] loss=0.02 avg=0.20\n",
            "[740 | 725.12] loss=0.03 avg=0.19\n",
            "[750 | 733.92] loss=0.05 avg=0.19\n",
            "[760 | 742.72] loss=0.05 avg=0.19\n",
            "[770 | 751.50] loss=0.04 avg=0.18\n",
            "[780 | 760.31] loss=0.02 avg=0.18\n",
            "[790 | 769.11] loss=0.04 avg=0.18\n",
            "[800 | 777.92] loss=0.02 avg=0.18\n",
            "======== SAMPLE 1 ========\n",
            " been in love with your wife all your life and loved her with all his soul, but he was a perfect creature, and loved the beautiful women as his wife did. That was the real tragedy of the Albigensias. They have loved their lives, but they have not made them their objects.\n",
            "\n",
            "I like persons better than principles, and I like persons with no principles better than anything else in the world.\n",
            "\n",
            "I like persons because they are worth believing in, because they are possible because there is something fundamentally in the idea of being possible that makes its way into everything that is made, that makes its progress through the universe tangible and measurable, that can be measured by its progress, and, therefore, that which has been is to be.'\n",
            "\n",
            "--Adès, Old Uncle, or, As I'm Going to Be, The Unknown, 1842\n",
            "\n",
            "I don't desire for anyone's name to be Meredith. I don't approve of it. But I don't think that the name 'Edith Piaf' explains her. To me, she is as much of a mystery as any of us are. The greater part of the people in Great Britain are poor inscrutable dullards. The few who are are worshipped at court, loved at once, or either with devotion and fear of the other. We are the only people in the world who can read each other. Every month, on average, we get by on the wages of a poet, a playwright, a dramatist, or a philosopher. A year has hardly passed since we have had a real marriage. I have seldom known any who were anything more than friendly acquaintances. There were also no marriages. The only people I have ever loved really liked me. I don't mean that in a bad way. On the contrary, I mean that I have never found it possible to hurt them. I have spent my life trying to make people feel. And wherever I have succeeded, they have always treated me with the greatest possible respect.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire any one's name. I don't like such things. I wish only to be understood. I wish only to be liked. I am not interested in men's names. I am interested in their objects.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything. I don't desire nothing.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't desire anything in the world. I don't desire anything in the world.\n",
            "\n",
            "--Adès, To the Amazons, 1842\n",
            "\n",
            "I don't\n",
            "\n",
            "[810 | 804.19] loss=0.02 avg=0.17\n",
            "[820 | 812.99] loss=0.03 avg=0.17\n",
            "[830 | 821.78] loss=0.01 avg=0.17\n",
            "[840 | 830.57] loss=0.02 avg=0.17\n",
            "[850 | 839.35] loss=0.02 avg=0.16\n",
            "[860 | 848.16] loss=0.02 avg=0.16\n",
            "[870 | 856.97] loss=0.01 avg=0.16\n",
            "[880 | 865.77] loss=0.04 avg=0.16\n",
            "[890 | 874.57] loss=0.01 avg=0.15\n",
            "[900 | 883.36] loss=0.02 avg=0.15\n",
            "[910 | 892.15] loss=0.02 avg=0.15\n",
            "[920 | 900.94] loss=0.02 avg=0.15\n",
            "[930 | 909.73] loss=0.05 avg=0.15\n",
            "[940 | 918.53] loss=0.03 avg=0.14\n",
            "[950 | 927.35] loss=0.06 avg=0.14\n",
            "[960 | 936.16] loss=0.01 avg=0.14\n",
            "[970 | 944.95] loss=0.03 avg=0.14\n",
            "[980 | 953.77] loss=0.02 avg=0.14\n",
            "[990 | 962.56] loss=0.03 avg=0.13\n",
            "[1000 | 971.36] loss=0.03 avg=0.13\n",
            "Saving checkpoint/lordhenry-355M/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='lordhenry-355M')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='lordhenry-355M')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "2aacbdd2-9835-4d2b-c6e0-9b1f157abd6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='lordhenry-355M')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/lordhenry-355M/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/lordhenry-355M/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "b9bfad8c-63e7-41cc-aa73-90f8ed23476c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='lordhenry-355M')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It was a beautiful summer's day in 1820, and the romantic face of the Duchess of Malfi was smiling down upon her vacation-goers. A flush of pride welled up in her brown orbs, and, casting her gaze adoringly over the countenances of her friends, she said--\"What charming people these are! How sociable they are! One should stalk their heels from place to place.\"The words left her lips, and burst into flames within her breast, as she thought of that enchanting young Darcy, that brilliant young John, that hussar's son, that last of the Regency sort. They seemed to her to beckon her irresistibly to follow them, and she seized on the jinx with all her might. She had, as it were, consorted with the cardinal's creatures, and it had been a mistake to trust her instincts. The moment she saw him, she dashed into his arms, and, burning with passion, she tore her hair like melons, and, throwing herself at his feet, she leaped into his arms with a crash like thunder. She was in love with him. He was astonished at the suddenness of his reaction, and, smiling sadly, had said nothing. When the days rolled on, and the flowers grew dim and faded, he had clung to life a little longer, had pressed on with all his might, had yawned and wheezed and wheezed, had lost himself in the memories of youth's youth's youth--had been disappointed. Now, when the days went on, and the years wore on, and the mental strength of the age failed it utterly, he had fallen headlong victim to despair. He had broken down, and, as the dark ages have done, he has just about gave them a sign of the devil's disapproval, he has just about given them the sign of his approval, he has just about gave them the sign of his love, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really, really\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "e2a84897-bcaf-4797-d7cb-1d41072ea928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              run_name = 'lordhenry-355M',\n",
        "              length=50,\n",
        "              temperature=0.9,\n",
        "              prefix=\"Influence\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Influence. He has it not. All influence is corruptive. Righteousness, morality, influence are passions, not science. The true art of life is in the practice of passive virtues. They are not the active principles of great deeds. Passion is\n",
            "====================\n",
            "Influence, at least as an ethical emotion, is a form of intellect that cannot be understood. It is that mysterious intelligence, that intangible spiritual essence that transcends experience and into which we are unable to penetrate, because we are intellectual creatures as well as animals\n",
            "====================\n",
            "Influence\n",
            "\n",
            "It is a fact of life that those for whose good we are in earnest strive day in and day out not to be, but to be the best that we can be. Think of the accidents that have befallen you if you had\n",
            "====================\n",
            "Influence is more precious to the mean than the great.\n",
            "\n",
            "Experience was of no ethical value. It was merely the name men gave to their mistakes. When a man sins he thinks on sin, and thinks on sin with thoughts of sin, and thinks\n",
            "====================\n",
            "Influence Magazine. Part one. Chapter seven.\n",
            "\n",
            "To obtain information in any other sense than that which our country has ennobled to the highest possible pitch of human excellence is a crime against the public good. It is, instead, the expression of\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}